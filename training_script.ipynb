{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c7eba35-fa35-42a5-ade6-f084cd1b08d2",
   "metadata": {},
   "source": [
    "A modified script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49186ac1-f248-4a43-abfc-e1c7e7733316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: /Users/zihuiouyang/.cache/modelscope/hub/models/mlx-community/DeepSeek-R1-Distill-Qwen-1.5B-bf16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 14:03:04,904 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['MLXLM_USE_MODELSCOPE'] = 'True'\n",
    "from mlx_lm import load, generate\n",
    "model, tokenizer = load(\"mlx-community/DeepSeek-R1-Distill-Qwen-1.5B-bf16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e8c1f-a4ee-492e-8f90-cdff0b247969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "folder=\"/Users/zihuiouyang/Documents/data/\" \n",
    "system_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "{schema}\"\"\"\n",
    "def create_conversation(sample):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n",
    "      {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
    "    ]\n",
    "  }\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n",
    "dataset = dataset.train_test_split(test_size=50/150)\n",
    "dataset_test_valid = dataset['test'].train_test_split(0.5)\n",
    "print(dataset[\"train\"][45][\"messages\"])\n",
    "dataset[\"train\"].to_json(folder + \"train.jsonl\", orient=\"records\")\n",
    "dataset_test_valid[\"train\"].to_json(folder + \"test.jsonl\", orient=\"records\")\n",
    "dataset_test_valid[\"test\"].to_json(folder + \"valid.jsonl\", orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0633d173-1617-4c60-9f71-3895f2fe1235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from mlx_lm.tuner import train, evaluate, TrainingArgs\n",
    "adapter_path = Path(\"adapters\")\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "lora_config = {\n",
    " \"num_layers\": 8,\n",
    " \"lora_parameters\": {\n",
    "    \"rank\": 8,\n",
    "    \"scale\": 20.0,\n",
    "    \"dropout\": 0.0,\n",
    "}}\n",
    "\n",
    "# Save the LoRA config to the adapter path\n",
    "with open(adapter_path / \"adapter_config.json\", \"w\") as fid:\n",
    "    json.dump(lora_config, fid, indent=4)    \n",
    "\n",
    "training_args = TrainingArgs(\n",
    "    adapter_file=adapter_path / \"adapters.safetensors\",\n",
    "    iters=13100,\n",
    "    steps_per_report=300,\n",
    "    steps_per_eval=500,\n",
    "    steps_per_save=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd674e0-28e5-40c6-957f-e787e1fe80c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 311296\n"
     ]
    }
   ],
   "source": [
    "from mlx.utils import tree_flatten\n",
    "from mlx_lm.tuner import linear_to_lora_layers\n",
    "model.freeze()\n",
    "\n",
    "# Convert linear layers to lora layers\n",
    "linear_to_lora_layers(model, lora_config[\"num_layers\"], lora_config[\"lora_parameters\"])\n",
    "\n",
    "num_train_params = (\n",
    "    sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
    ")\n",
    "print(f\"Number of trainable parameters: {num_train_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44435185-9d03-4816-9e36-152e2769cc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/zihuiouyang/Documents/data/train.jsonl\", \"r\") as fid:\n",
    "        train_data = [json.loads(line) for line in fid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95c5caf7-ed30-425a-aa3e-9cabeb041e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/zihuiouyang/Documents/data/valid.jsonl\", \"r\") as fid:\n",
    "        valid_data = [json.loads(line) for line in fid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be83519a-cf34-4b1b-9c16-7b94d7bb22c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm.tuner.datasets import ChatDataset\n",
    "def make_dataset(ds):\n",
    "    return ChatDataset(\n",
    "        ds,\n",
    "        tokenizer\n",
    "    )\n",
    "train_set, valid_set = make_dataset(train_data), make_dataset(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e2be8f8-89af-4c10-b98c-55734f2620c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.optimizers as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfd9d469-e27d-4a3a-9c01-154f6cd57d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the model in training mode:\n",
    "model.train()\n",
    "\n",
    "# Make the optimizer:\n",
    "opt = optim.Adam(learning_rate=1e-5)\n",
    "\n",
    "# Make a class to record the training stats:\n",
    "class Metrics:\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    def on_train_loss_report(self, info):\n",
    "        self.train_losses.append((info[\"iteration\"], info[\"train_loss\"]))\n",
    "    def on_val_loss_report(self, info):\n",
    "        self.val_losses.append((info[\"iteration\"], info[\"val_loss\"]))\n",
    "\n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04fe0e05-6067-4c2d-9b22-11434fc0b27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training..., iters: 13100\n",
      "Iter 1: Val loss 4.576, Val took 11.159s\n",
      "Iter 300: Train loss 2.092, Learning Rate 1.000e-05, It/sec 1.555, Tokens/sec 611.775, Trained Tokens 118055, Peak mem 7.269 GB\n",
      "Iter 500: Val loss 1.405, Val took 10.397s\n",
      "Iter 600: Train loss 1.495, Learning Rate 1.000e-05, It/sec 1.586, Tokens/sec 616.086, Trained Tokens 234573, Peak mem 7.269 GB\n",
      "Iter 900: Train loss 1.417, Learning Rate 1.000e-05, It/sec 1.582, Tokens/sec 609.713, Trained Tokens 350199, Peak mem 7.269 GB\n",
      "Iter 1000: Val loss 1.358, Val took 10.764s\n",
      "Iter 1000: Saved adapter weights to adapters/adapters.safetensors and adapters/0001000_adapters.safetensors.\n",
      "Iter 1200: Train loss 1.368, Learning Rate 1.000e-05, It/sec 1.597, Tokens/sec 615.101, Trained Tokens 465775, Peak mem 7.269 GB\n",
      "Iter 1500: Val loss 1.416, Val took 11.091s\n",
      "Iter 1500: Train loss 1.339, Learning Rate 1.000e-05, It/sec 1.585, Tokens/sec 616.100, Trained Tokens 582362, Peak mem 7.269 GB\n",
      "Iter 1800: Train loss 1.337, Learning Rate 1.000e-05, It/sec 1.599, Tokens/sec 618.312, Trained Tokens 698402, Peak mem 7.269 GB\n",
      "Iter 2000: Val loss 1.273, Val took 10.664s\n",
      "Iter 2000: Saved adapter weights to adapters/adapters.safetensors and adapters/0002000_adapters.safetensors.\n",
      "Iter 2100: Train loss 1.283, Learning Rate 1.000e-05, It/sec 1.601, Tokens/sec 614.565, Trained Tokens 813571, Peak mem 7.269 GB\n",
      "Iter 2400: Train loss 1.273, Learning Rate 1.000e-05, It/sec 1.601, Tokens/sec 618.736, Trained Tokens 929530, Peak mem 7.269 GB\n",
      "Iter 2500: Val loss 1.276, Val took 10.835s\n",
      "Iter 2700: Train loss 1.249, Learning Rate 1.000e-05, It/sec 1.579, Tokens/sec 616.577, Trained Tokens 1046693, Peak mem 7.269 GB\n",
      "Iter 3000: Val loss 1.281, Val took 11.123s\n",
      "Iter 3000: Train loss 1.255, Learning Rate 1.000e-05, It/sec 1.553, Tokens/sec 615.776, Trained Tokens 1165643, Peak mem 7.269 GB\n",
      "Iter 3000: Saved adapter weights to adapters/adapters.safetensors and adapters/0003000_adapters.safetensors.\n",
      "Iter 3300: Train loss 1.215, Learning Rate 1.000e-05, It/sec 1.595, Tokens/sec 617.954, Trained Tokens 1281879, Peak mem 7.269 GB\n",
      "Iter 3500: Val loss 1.214, Val took 11.098s\n",
      "Iter 3600: Train loss 1.172, Learning Rate 1.000e-05, It/sec 1.621, Tokens/sec 616.446, Trained Tokens 1395942, Peak mem 7.269 GB\n",
      "Iter 3900: Train loss 1.212, Learning Rate 1.000e-05, It/sec 1.584, Tokens/sec 616.446, Trained Tokens 1512685, Peak mem 7.269 GB\n",
      "Iter 4000: Val loss 1.206, Val took 10.518s\n",
      "Iter 4000: Saved adapter weights to adapters/adapters.safetensors and adapters/0004000_adapters.safetensors.\n",
      "Iter 4200: Train loss 1.189, Learning Rate 1.000e-05, It/sec 1.580, Tokens/sec 616.654, Trained Tokens 1629756, Peak mem 7.269 GB\n",
      "Iter 4500: Val loss 1.133, Val took 10.449s\n",
      "Iter 4500: Train loss 1.183, Learning Rate 1.000e-05, It/sec 1.569, Tokens/sec 615.967, Trained Tokens 1747551, Peak mem 7.269 GB\n",
      "Iter 4800: Train loss 1.177, Learning Rate 1.000e-05, It/sec 1.563, Tokens/sec 616.602, Trained Tokens 1865882, Peak mem 7.269 GB\n",
      "Iter 5000: Val loss 1.164, Val took 10.575s\n",
      "Iter 5000: Saved adapter weights to adapters/adapters.safetensors and adapters/0005000_adapters.safetensors.\n",
      "Iter 5100: Train loss 1.162, Learning Rate 1.000e-05, It/sec 1.588, Tokens/sec 617.925, Trained Tokens 1982582, Peak mem 7.269 GB\n",
      "Iter 5400: Train loss 1.162, Learning Rate 1.000e-05, It/sec 1.586, Tokens/sec 616.733, Trained Tokens 2099240, Peak mem 7.269 GB\n",
      "Iter 5500: Val loss 1.142, Val took 10.465s\n",
      "Iter 5700: Train loss 1.159, Learning Rate 1.000e-05, It/sec 1.580, Tokens/sec 616.849, Trained Tokens 2216365, Peak mem 7.269 GB\n",
      "Iter 6000: Val loss 1.188, Val took 10.719s\n",
      "Iter 6000: Train loss 1.159, Learning Rate 1.000e-05, It/sec 1.579, Tokens/sec 614.721, Trained Tokens 2333170, Peak mem 7.269 GB\n",
      "Iter 6000: Saved adapter weights to adapters/adapters.safetensors and adapters/0006000_adapters.safetensors.\n",
      "Iter 6300: Train loss 1.163, Learning Rate 1.000e-05, It/sec 1.601, Tokens/sec 618.338, Trained Tokens 2449053, Peak mem 7.269 GB\n",
      "Iter 6500: Val loss 1.155, Val took 11.157s\n",
      "Iter 6600: Train loss 1.150, Learning Rate 1.000e-05, It/sec 1.582, Tokens/sec 614.309, Trained Tokens 2565519, Peak mem 7.269 GB\n",
      "Iter 6900: Train loss 1.160, Learning Rate 1.000e-05, It/sec 1.582, Tokens/sec 617.645, Trained Tokens 2682616, Peak mem 7.269 GB\n",
      "Iter 7000: Val loss 1.206, Val took 11.311s\n",
      "Iter 7000: Saved adapter weights to adapters/adapters.safetensors and adapters/0007000_adapters.safetensors.\n",
      "Iter 7200: Train loss 1.147, Learning Rate 1.000e-05, It/sec 1.579, Tokens/sec 617.524, Trained Tokens 2799945, Peak mem 7.269 GB\n",
      "Iter 7500: Val loss 1.100, Val took 10.448s\n",
      "Iter 7500: Train loss 1.146, Learning Rate 1.000e-05, It/sec 1.582, Tokens/sec 616.784, Trained Tokens 2916879, Peak mem 7.269 GB\n",
      "Iter 7800: Train loss 1.117, Learning Rate 1.000e-05, It/sec 1.625, Tokens/sec 614.425, Trained Tokens 3030323, Peak mem 7.269 GB\n",
      "Iter 8000: Val loss 1.087, Val took 10.606s\n",
      "Iter 8000: Saved adapter weights to adapters/adapters.safetensors and adapters/0008000_adapters.safetensors.\n",
      "Iter 8100: Train loss 1.135, Learning Rate 1.000e-05, It/sec 1.588, Tokens/sec 614.607, Trained Tokens 3146411, Peak mem 11.084 GB\n",
      "Iter 8400: Train loss 1.141, Learning Rate 1.000e-05, It/sec 1.588, Tokens/sec 618.344, Trained Tokens 3263252, Peak mem 11.084 GB\n",
      "Iter 8500: Val loss 1.129, Val took 11.381s\n",
      "Iter 8700: Train loss 1.109, Learning Rate 1.000e-05, It/sec 1.623, Tokens/sec 615.705, Trained Tokens 3377094, Peak mem 11.084 GB\n",
      "Iter 9000: Val loss 1.140, Val took 10.569s\n",
      "Iter 9000: Train loss 1.123, Learning Rate 1.000e-05, It/sec 1.586, Tokens/sec 614.480, Trained Tokens 3493355, Peak mem 11.084 GB\n",
      "Iter 9000: Saved adapter weights to adapters/adapters.safetensors and adapters/0009000_adapters.safetensors.\n",
      "Iter 9300: Train loss 1.141, Learning Rate 1.000e-05, It/sec 1.582, Tokens/sec 621.853, Trained Tokens 3611247, Peak mem 11.084 GB\n",
      "Iter 9500: Val loss 1.159, Val took 10.531s\n",
      "Iter 9600: Train loss 1.112, Learning Rate 1.000e-05, It/sec 1.590, Tokens/sec 613.850, Trained Tokens 3727039, Peak mem 11.084 GB\n",
      "Iter 9900: Train loss 1.119, Learning Rate 1.000e-05, It/sec 1.576, Tokens/sec 617.521, Trained Tokens 3844600, Peak mem 11.084 GB\n",
      "Iter 10000: Val loss 1.084, Val took 10.555s\n",
      "Iter 10000: Saved adapter weights to adapters/adapters.safetensors and adapters/0010000_adapters.safetensors.\n",
      "Iter 10200: Train loss 1.104, Learning Rate 1.000e-05, It/sec 1.585, Tokens/sec 610.956, Trained Tokens 3960215, Peak mem 11.084 GB\n",
      "Iter 10500: Val loss 1.119, Val took 11.022s\n",
      "Iter 10500: Train loss 1.117, Learning Rate 1.000e-05, It/sec 1.552, Tokens/sec 609.456, Trained Tokens 4078060, Peak mem 11.084 GB\n",
      "Iter 10800: Train loss 1.109, Learning Rate 1.000e-05, It/sec 1.594, Tokens/sec 614.274, Trained Tokens 4193681, Peak mem 11.084 GB\n",
      "Iter 11000: Val loss 1.161, Val took 10.995s\n",
      "Iter 11000: Saved adapter weights to adapters/adapters.safetensors and adapters/0011000_adapters.safetensors.\n",
      "Iter 11100: Train loss 1.101, Learning Rate 1.000e-05, It/sec 1.599, Tokens/sec 616.087, Trained Tokens 4309248, Peak mem 11.084 GB\n",
      "Iter 11400: Train loss 1.110, Learning Rate 1.000e-05, It/sec 1.597, Tokens/sec 618.405, Trained Tokens 4425448, Peak mem 11.084 GB\n",
      "Iter 11500: Val loss 1.146, Val took 11.188s\n",
      "Iter 11700: Train loss 1.118, Learning Rate 1.000e-05, It/sec 1.573, Tokens/sec 614.879, Trained Tokens 4542749, Peak mem 11.084 GB\n",
      "Iter 12000: Val loss 1.108, Val took 10.672s\n",
      "Iter 12000: Train loss 1.100, Learning Rate 1.000e-05, It/sec 1.564, Tokens/sec 618.114, Trained Tokens 4661306, Peak mem 11.084 GB\n",
      "Iter 12000: Saved adapter weights to adapters/adapters.safetensors and adapters/0012000_adapters.safetensors.\n",
      "Iter 12300: Train loss 1.088, Learning Rate 1.000e-05, It/sec 1.608, Tokens/sec 614.476, Trained Tokens 4775959, Peak mem 11.084 GB\n",
      "Iter 12500: Val loss 1.117, Val took 10.817s\n",
      "Iter 12600: Train loss 1.109, Learning Rate 1.000e-05, It/sec 1.568, Tokens/sec 614.386, Trained Tokens 4893532, Peak mem 11.084 GB\n",
      "Iter 12900: Train loss 1.083, Learning Rate 1.000e-05, It/sec 1.644, Tokens/sec 616.343, Trained Tokens 5006027, Peak mem 11.084 GB\n",
      "Iter 13000: Val loss 1.109, Val took 10.598s\n",
      "Iter 13000: Saved adapter weights to adapters/adapters.safetensors and adapters/0013000_adapters.safetensors.\n",
      "Iter 13100: Val loss 1.118, Val took 10.578s\n",
      "Iter 13100: Train loss 1.085, Learning Rate 1.000e-05, It/sec 2.391, Tokens/sec 610.901, Trained Tokens 5082663, Peak mem 11.084 GB\n",
      "Saved final weights to adapters/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    optimizer=opt,\n",
    "    train_dataset=train_set,\n",
    "    val_dataset=valid_set,\n",
    "    training_callback=metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e83b28d-fd78-43d6-be39-b586dc7c8758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    }
   ],
   "source": [
    "!mlx_lm.fuse --model /Users/zihuiouyang/.cache/modelscope/hub/models/mlx-community/DeepSeek-R1-Distill-Qwen-1.5B-bf16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b495a47d-1ab4-479f-a451-f4abd6d01398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Okay, so I need to translate the user's query into a SQL query based on the provided schema. The user asked, \"Tell me the score of misa eguchi eri hozumi.\" First, I should figure out what the user is asking for. It seems like they want the score of a specific player named \"misa eguchi eri hozumi.\" \n",
      "\n",
      " at the schema, there's a table named table_name_62 with two columns: score and opponents. The user is asking for the score, so I need to select the score column. The name of the player is a bit unusual, but I can assume it's a single name. \n",
      "\n",
      "I should make sure the SQL query selects the score from the table. The correct syntax would be SELECT score FROM table_name_62. Since the user didn't specify the table name, I'll leave it as table_name_62. \n",
      "\n",
      "'t think the opponents column is needed here because the user only asked for the score. So, the final SQL query should just select the score from the table. I'll write it out clearly so the user understands exactly what they're getting.\n",
      "</think>\n",
      "\n",
      " FROM table_name_62\n",
      "==========\n",
      "Prompt: 71 tokens, 366.390 tokens-per-sec\n",
      "Generation: 250 tokens, 47.717 tokens-per-sec\n",
      "Peak memory: 3.651 GB\n"
     ]
    }
   ],
   "source": [
    "!mlx_lm.generate --max-tokens 2048 --model /Users/zihuiouyang/.cache/modelscope/hub/models/mlx-community/DeepSeek-R1-Distill-Qwen-1.5B-bf16 --prompt \"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_62 (score VARCHAR, opponents VARCHAR)\\nUser:Tell me the score of misa eguchi eri hozumi\\nAssistant:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171a4b73-1463-469c-8d94-f1a9f3f009fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
